# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGxPKjnj1lS-YIYCRMn9dqBOinJhv1VW
"""

!pip install -q nltk scikit-learn matplotlib seaborn imbalanced-learn wordcloud spacy transformers sentence-transformers lime
!python -m spacy download en_core_web_sm
import nltk
nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')

from google.colab import files
uploaded = files.upload()  # yahan Fake.csv aur True.csv dono upload karo

import pandas as pd
df = pd.read_csv("fake_or_real_news.csv")   # or your file
print(df.shape)
df.head()
df['label'].value_counts(normalize=True)   # class balance
df['text_length'] = df['text'].str.split().str.len()
df['text_length'].hist(bins=50)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
stop = set(stopwords.words('english'))
lemm = WordNetLemmatizer()

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www.\S+', '', text)         # remove urls
    text = re.sub(r'\S+@\S+', '', text)                 # remove emails
    text = re.sub(r'[^a-z\s]', ' ', text)               # keep letters
    tokens = nltk.word_tokenize(text)
    tokens = [lemm.lemmatize(t) for t in tokens if t not in stop and len(t)>2]
    return " ".join(tokens)

if 'df' in globals():
    df['clean_text'] = df['text'].apply(clean_text)
else:
    print("DataFrame 'df' not found. Please make sure the previous cell ran successfully.")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

if 'df' in globals() and 'clean_text' in df.columns:
    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))  # unigrams + bigrams
    X = tfidf.fit_transform(df['clean_text'])
    y = df['label'].map({'REAL':0,'FAKE':1})   # convert to 0/1

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

else:
    print("DataFrame 'df' or 'clean_text' column not found. Please make sure the previous cells ran successfully.")

from sklearn.linear_model import LogisticRegression
if 'X_train' in globals() and 'y_train' in globals():
    model = LogisticRegression(class_weight='balanced', max_iter=1000)
    model.fit(X_train, y_train)
else:
    print("Training data (X_train, y_train) not found. Please make sure the previous cell ran successfully.")

from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

models = {
    'NB': MultinomialNB(),
    'Logistic': LogisticRegression(max_iter=1000, class_weight='balanced'),
    'SVM': LinearSVC(class_weight='balanced')
}

if 'X_train' in globals() and 'y_train' in globals() and 'X_test' in globals() and 'y_test' in globals():
    for name, m in models.items():
        m.fit(X_train, y_train)
        preds = m.predict(X_test)
        print(name)
        print(classification_report(y_test, preds))
else:
    print("Training or testing data not found. Please make sure the previous cells ran successfully.")

from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc
import matplotlib.pyplot as plt

if 'y_test' in globals() and 'preds' in globals():
    # confusion
    ConfusionMatrixDisplay.from_predictions(y_test, preds)
    plt.show()
else:
    print("Testing data (y_test) or predictions (preds) not found. Please make sure the previous cells ran successfully.")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Predict probabilities (for ROC we need probabilities, not just 0/1)
y_probs = model.predict_proba(X_test)[:, 1]

fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color="blue", label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0,1], [0,1], color="gray", linestyle="--")  # random guess line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

!pip install lime
from lime.lime_text import LimeTextExplainer

if 'df' in globals() and 'clean_text' in df.columns and 'models' in globals() and 'Logistic' in models and 'tfidf' in globals():
    explainer = LimeTextExplainer(class_names=['REAL','FAKE'])
    exp = explainer.explain_instance(
        df['clean_text'].iloc[10],
        lambda x: models['Logistic'].predict_proba(tfidf.transform(x)),  # Vectorize input here
        num_features=6
    )
    exp.show_in_notebook(text=df['clean_text'].iloc[10]) # Pass original text to show_in_notebook
else:
    print("DataFrame 'df' or 'clean_text' column or models or tfidf not found. Please make sure the previous cells ran successfully.")

from transformers import pipeline
clf = pipeline("text-classification", model="microsoft/deberta-base-mnli")  # example
res = clf("Some news article text")